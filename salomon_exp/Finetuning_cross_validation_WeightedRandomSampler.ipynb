{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pretrainedmodels in /home/ubuntu/anaconda3/lib/python3.6/site-packages (0.7.4)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pretrainedmodels) (4.45.0)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pretrainedmodels) (1.5.0)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pretrainedmodels) (0.6.0)\n",
      "Requirement already satisfied: munch in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pretrainedmodels) (2.5.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from torch->pretrainedmodels) (1.16.4)\n",
      "Requirement already satisfied: future in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from torch->pretrainedmodels) (0.18.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (5.4.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from munch->pretrainedmodels) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "d3cVbqIs7PDi",
    "outputId": "19f10d20-d20f-4a31-d1d4-b97da869c7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.4.0\n",
      "Torchvision Version:  0.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-16GB'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "#import pretrainedmodels\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "import pretrainedmodels\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB5I0Rw47gBn"
   },
   "outputs": [],
   "source": [
    "# print('Train set:')\n",
    "# for cls in os.listdir(\"../input/ammi-2020-convnets/train/train\"):\n",
    "#     print('{}:{}'.format(cls, len(os.listdir(os.path.join(\"../input/ammi-2020-convnets/train/train\", cls)))))\n",
    "# im = Image.open('../input/ammi-2020-convnets/train/train/cgm/train-cgm-738.jpg')\n",
    "# print(im.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60SznP4x7qZb"
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/train/train\"\n",
    "test_path = \"./data/test/test\"\n",
    "extraimage_path = \"./data/extraimages/extraimages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "cmd:2658\n",
      "healthy:316\n",
      "cbsd:1443\n",
      "cbb:466\n",
      "cgm:773\n",
      "(500, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cmd': 2658, 'healthy': 316, 'cbsd': 1443, 'cbb': 466, 'cgm': 773}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train set:')\n",
    "class_distrbution = {}\n",
    "for cls in os.listdir(data_path):\n",
    "    print('{}:{}'.format(cls, len(os.listdir(os.path.join(data_path, cls)))))\n",
    "    class_distrbution[cls] =  len(os.listdir(os.path.join(data_path, cls)))\n",
    "im = Image.open(data_path+'/cgm/train-cgm-738.jpg')\n",
    "print(im.size)\n",
    "class_distrbution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq7aPxZO8Nyc"
   },
   "source": [
    "# Distribution of the classes in the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BKbVY308Ss2"
   },
   "outputs": [],
   "source": [
    "# Transformations for both the training and testing data\n",
    "\n",
    "# Transformations for both the training and testing data\n",
    "mean=[0.4543, 0.5137, 0.3240]\n",
    "std=[0.1949, 0.1977, 0.1661]\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224), #448, 299, 224, 331\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean,std=std)])\n",
    "\n",
    "test_transforms = transforms.Compose([ transforms.Resize(224),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean,std=std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9Za0HGZ8W4F"
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.classes = os.listdir(path)\n",
    "        self.path = [f\"{path}/{className}\" for className in self.classes]\n",
    "        self.file_list = [glob.glob(f\"{x}/*\") for x in self.path]\n",
    "        self.transform = transform\n",
    "\n",
    "        files = []\n",
    "        class_names = {}\n",
    "        for i, className in enumerate(self.classes):\n",
    "            for fileName in self.file_list[i]:\n",
    "                files.append([i, className, fileName])\n",
    "\n",
    "                name = str(i)+'-'+className\n",
    "                if name not in class_names:\n",
    "                    class_names[name] = 1\n",
    "                else:\n",
    "                    class_names[name] += 1\n",
    "        self.file_list = files\n",
    "#         print(class_names)\n",
    "        files = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fileName = self.file_list[idx][2]\n",
    "        classCategory = self.file_list[idx][0]\n",
    "        im = Image.open(fileName)\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "# #         return im.view(3, 448, 448), classCategory\n",
    "        return im.view(3, 224, 224), classCategory\n",
    "# #         return im.view(3, 299, 299), classCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0jldgqi8aqT"
   },
   "outputs": [],
   "source": [
    "train_data = CassavaDataset(data_path, transform=train_transforms)\n",
    "\n",
    "test_data = CassavaDataset(test_path, transform=test_transforms)\n",
    "\n",
    "extraimage_data = CassavaDataset(extraimage_path, transform=train_transforms) #maybe need an other trasforms, I had to change the dataset structure :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "validation_split = 0.0\n",
    "shuffle_dataset = True\n",
    "random_seed= 42 #42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "batch_size = 125 #16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                             sampler=train_sampler)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "#                                              sampler=valid_sampler)\n",
    "\n",
    "unlabeled_loader = torch.utils.data.DataLoader(extraimage_data, batch_size=batch_size) # to make batch_size work, I had to moove all the unlabeled data in a 0 folder\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1) # make batch = 1 here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_train = 0\n",
    "# from collections import Counter\n",
    "\n",
    "# Classes = Counter()\n",
    "\n",
    "# for i, (data, label) in enumerate(train_loader):\n",
    "#     total_train += i\n",
    "#     Classes.update(label.tolist())# += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5656"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "np.set_printoptions(precision=6)\n",
    "# torch.set_printoptions(precision=6)\n",
    "torch.set_printoptions(precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_2d_numpy(dataset):\n",
    "    np.set_printoptions(precision=6)\n",
    "    torch.set_printoptions(precision=6)\n",
    "    \n",
    "    batch_size = dataset.__len__() # to take the full data\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    \n",
    "    for (X_hat, y_hat) in train_loader:\n",
    "        X = X_hat.view(batch_size, -1)\n",
    "        X = X.numpy()\n",
    "        y = y_hat.numpy()\n",
    "    return X, y\n",
    "\n",
    "def tensor_to_2d_numpy(dataset):\n",
    "    np.set_printoptions(precision=6)\n",
    "    torch.set_printoptions(precision=6)\n",
    "    \n",
    "    batch_size = dataset.__len__() # to take the full data\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    \n",
    "    for (X_hat, y_hat) in train_loader:\n",
    "        X_org = X_hat\n",
    "        X = X_hat.view(batch_size, -1)\n",
    "        X = X.numpy()\n",
    "        y = y_hat.numpy()\n",
    "    return X, y, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tensor_to_2d_numpy(train_data)\n",
    "\n",
    "# X, y, X__ = tensor_to_2d_numpy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_2d_numpy_back_to_tensor(X, y, img_size = 224):\n",
    "    np.set_printoptions(precision=6)\n",
    "    torch.set_printoptions(precision=6)\n",
    "    \n",
    "    X_hat = torch.from_numpy(X)\n",
    "    \n",
    "    y_hat = torch.from_numpy(y)\n",
    "    \n",
    "    X_hat = X_hat.view(X.shape[0], 3, 224, -1)\n",
    "\n",
    "#     batch_size = dataset.__len__() # to take the full data\n",
    "    \n",
    "#     train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    \n",
    "#     for (X_hat, y_hat) in train_loader:\n",
    "#         X = X_hat.view(batch_size, -1)\n",
    "#         X = X.numpy()\n",
    "#         y = y_hat.numpy()\n",
    "    return X_hat, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX_, _ = from_2d_numpy_back_to_tensor(X, y, img_size = 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XXX_.equal(X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656, 150528)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.combine import SMOTETomek # doctest: +NORMALIZE_WHITESPACE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 2658, 2: 1443, 4: 773, 3: 466, 1: 316})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656, 150528)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = smt.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 2658, 1: 2658, 2: 2658, 3: 2658, 4: 2658})\n"
     ]
    }
   ],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13290, 150528)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13290,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_2d_numpy_back_to_tensor(X, y, img_size = 224):\n",
    "    np.set_printoptions(precision=6)\n",
    "    torch.set_printoptions(precision=6)\n",
    "    \n",
    "    X_hat = torch.from_numpy(X)\n",
    "    \n",
    "    y_hat = torch.from_numpy(y)\n",
    "    \n",
    "    X_hat = X_hat.view(X.shape[0], 3, 224, -1)\n",
    "\n",
    "#     batch_size = dataset.__len__() # to take the full data\n",
    "    \n",
    "#     train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    \n",
    "#     for (X_hat, y_hat) in train_loader:\n",
    "#         X = X_hat.view(batch_size, -1)\n",
    "#         X = X.numpy()\n",
    "#         y = y_hat.numpy()\n",
    "    return X_hat, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch, y_torch = from_2d_numpy_back_to_tensor(X, y, img_size = 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5656, 3, 224, 224])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5656])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "# tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "num_workers = 2\n",
    "# how many samples per batch to load\n",
    "batch_size = 16\n",
    "\n",
    "my_dataset = torch.utils.data.TensorDataset(X_torch, y_torch) # create your datset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(my_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean=[0.4543, 0.5137, 0.3240]\n",
    "# std=[0.1949, 0.1977, 0.1661]\n",
    "\n",
    "# inv_normalize = transforms.Normalize(\n",
    "#    mean=[-0.4543/0.1949, -0.5137/0.1977, -0.3240/0.1661],\n",
    "#    std=[1/0.1949, 1/0.1977, 1/0.1661]\n",
    "# )\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std=[1/s for s in std]\n",
    ")\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = inv_normalize(img)# / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# matplotlib_imshow(img_grid, one_channel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Additional(nn.Module):\n",
    "    def __init__(self, modelA,in_features,nb_classes=5, freeze = False):\n",
    "        super(Additional, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        # Remove last linear layer\n",
    "#         self.modelA.fc = nn.Identity() # for resnet\n",
    "        self.modelA.last_linear = nn.Identity() #for re_renext\n",
    "#         self.modelA.classifier = nn.Identity()    # densenet201\n",
    "        for p in self.modelA.parameters():\n",
    "            if freeze:\n",
    "                p.requires_grad = False\n",
    "            else :\n",
    "                p.requires_grad = True\n",
    "        \n",
    "        # Create new classifier\n",
    "        self.fc_1 = nn.Linear(in_features,256)\n",
    "        self.fc_2 = nn.Linear(256,  512)\n",
    "        self.fc_out = nn.Linear( 512, nb_classes)\n",
    "        \n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #model\n",
    "        x = self.modelA(x.clone())  \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #FC\n",
    "        x  = self.dropout(self.fc_1(F.relu(x)))\n",
    "        x = self.dropout(self.fc_2(F.relu(x)))\n",
    "        x = self.fc_out(F.relu(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used during training process, to calculation the loss and accuracy\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train, total_acc_train = [],[]\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        N = images.size(0)\n",
    "        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "    return train_loss.avg, train_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, optimizer, epoch):\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'se_resnext101_32x4d' # se_resnext101_32x4d, resnext101_64x4d, nasnetlarge\n",
    "# resnet_model = torch.hub.load('pytorch/vision:v0.5.0', model_name, pretrained=True)\n",
    "resnet_model = pretrainedmodels.se_resnext101_32x4d(num_classes=1000, pretrained=\"imagenet\")# todo : how to pretrained=False ?\n",
    "\n",
    "# resnet_model = torchvision.models.resnet50(pretrained=True)\n",
    "#---------------------------------------------\n",
    "\n",
    "# num_fits = resnet_model.fc.in_features\n",
    "num_fits = resnet_model.last_linear.in_features # se_resnext101_32x4d\n",
    "# num_fits = resnet_model.classifier.in_features # densenet201\n",
    "num_fits\n",
    "\n",
    "\n",
    "model = Additional(resnet_model, num_fits, freeze = False)\n",
    "model = model.to(device)\n",
    "# model\n",
    "\n",
    "#---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "          ------------- Fold 0 -----------\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [iter 100 / 283], [train loss 1.25230], [train acc 0.53438]\n",
      "[epoch 1], [iter 200 / 283], [train loss 1.05290], [train acc 0.61531]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 0.33100], [val acc 0.90405]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 1], [val loss 0.33100], [val acc 0.90405]\n",
      "*****************************************************\n",
      "[epoch 2], [iter 100 / 283], [train loss 0.71723], [train acc 0.77938]\n",
      "[epoch 2], [iter 200 / 283], [train loss 0.67835], [train acc 0.78375]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 0.33409], [val acc 0.90023]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [iter 100 / 283], [train loss 0.62476], [train acc 0.79875]\n",
      "[epoch 3], [iter 200 / 283], [train loss 0.61070], [train acc 0.80031]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 0.26044], [val acc 0.91667]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 3], [val loss 0.26044], [val acc 0.91667]\n",
      "*****************************************************\n",
      "[epoch 4], [iter 100 / 283], [train loss 0.59424], [train acc 0.80375]\n",
      "[epoch 4], [iter 200 / 283], [train loss 0.58131], [train acc 0.80563]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 0.29784], [val acc 0.91285]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [iter 100 / 283], [train loss 0.54226], [train acc 0.81625]\n",
      "[epoch 5], [iter 200 / 283], [train loss 0.53791], [train acc 0.82250]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 0.36986], [val acc 0.86972]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [iter 100 / 283], [train loss 0.52781], [train acc 0.82375]\n",
      "[epoch 6], [iter 200 / 283], [train loss 0.52524], [train acc 0.82688]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 0.20670], [val acc 0.94718]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 6], [val loss 0.20670], [val acc 0.94718]\n",
      "*****************************************************\n",
      "[epoch 7], [iter 100 / 283], [train loss 0.49861], [train acc 0.84313]\n",
      "[epoch 7], [iter 200 / 283], [train loss 0.51412], [train acc 0.83813]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 0.32210], [val acc 0.91168]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [iter 100 / 283], [train loss 0.49052], [train acc 0.83813]\n",
      "[epoch 8], [iter 200 / 283], [train loss 0.48999], [train acc 0.84125]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 0.31193], [val acc 0.90023]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [iter 100 / 283], [train loss 0.48090], [train acc 0.84625]\n",
      "[epoch 9], [iter 200 / 283], [train loss 0.46644], [train acc 0.85344]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 0.34181], [val acc 0.89231]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [iter 100 / 283], [train loss 0.43400], [train acc 0.86313]\n",
      "[epoch 10], [iter 200 / 283], [train loss 0.45748], [train acc 0.85719]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 0.33184], [val acc 0.88938]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [iter 100 / 283], [train loss 0.44233], [train acc 0.85625]\n",
      "[epoch 11], [iter 200 / 283], [train loss 0.44497], [train acc 0.85562]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [val loss 0.24562], [val acc 0.93574]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [iter 100 / 283], [train loss 0.41416], [train acc 0.86187]\n",
      "[epoch 12], [iter 200 / 283], [train loss 0.43505], [train acc 0.85875]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [val loss 0.32126], [val acc 0.89994]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [iter 100 / 283], [train loss 0.41082], [train acc 0.87438]\n",
      "[epoch 13], [iter 200 / 283], [train loss 0.40817], [train acc 0.87250]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [val loss 0.21290], [val acc 0.94542]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [iter 100 / 283], [train loss 0.39775], [train acc 0.87313]\n",
      "[epoch 14], [iter 200 / 283], [train loss 0.37404], [train acc 0.88344]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [val loss 0.43857], [val acc 0.86502]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [iter 100 / 283], [train loss 0.37986], [train acc 0.87375]\n",
      "[epoch 15], [iter 200 / 283], [train loss 0.39711], [train acc 0.87531]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [val loss 0.32064], [val acc 0.90200]\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "          ------------- Fold 1 -----------\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [iter 100 / 283], [train loss 0.86896], [train acc 0.77875]\n",
      "[epoch 1], [iter 200 / 283], [train loss 0.71164], [train acc 0.81469]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 0.24442], [val acc 0.96791]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 1], [val loss 0.24442], [val acc 0.96791]\n",
      "*****************************************************\n",
      "[epoch 2], [iter 100 / 283], [train loss 0.46161], [train acc 0.87062]\n",
      "[epoch 2], [iter 200 / 283], [train loss 0.46490], [train acc 0.86531]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 0.14906], [val acc 0.96215]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [iter 100 / 283], [train loss 0.45445], [train acc 0.87000]\n",
      "[epoch 3], [iter 200 / 283], [train loss 0.44556], [train acc 0.87500]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 0.22839], [val acc 0.94102]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [iter 100 / 283], [train loss 0.42824], [train acc 0.87250]\n",
      "[epoch 4], [iter 200 / 283], [train loss 0.45220], [train acc 0.86625]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 0.18043], [val acc 0.95687]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [iter 100 / 283], [train loss 0.37264], [train acc 0.89250]\n",
      "[epoch 5], [iter 200 / 283], [train loss 0.39403], [train acc 0.88469]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 0.18227], [val acc 0.95246]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [iter 100 / 283], [train loss 0.36565], [train acc 0.89187]\n",
      "[epoch 6], [iter 200 / 283], [train loss 0.37627], [train acc 0.88938]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 0.18220], [val acc 0.95511]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [iter 100 / 283], [train loss 0.36700], [train acc 0.88438]\n",
      "[epoch 7], [iter 200 / 283], [train loss 0.38431], [train acc 0.88125]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 0.19881], [val acc 0.95471]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [iter 100 / 283], [train loss 0.35703], [train acc 0.89687]\n",
      "[epoch 8], [iter 200 / 283], [train loss 0.36058], [train acc 0.89250]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 0.28277], [val acc 0.91989]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [iter 100 / 283], [train loss 0.33661], [train acc 0.89312]\n",
      "[epoch 9], [iter 200 / 283], [train loss 0.35929], [train acc 0.88938]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 0.16640], [val acc 0.95647]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [iter 100 / 283], [train loss 0.30030], [train acc 0.91563]\n",
      "[epoch 10], [iter 200 / 283], [train loss 0.32777], [train acc 0.90312]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 0.20673], [val acc 0.93846]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [iter 100 / 283], [train loss 0.31589], [train acc 0.90438]\n",
      "[epoch 11], [iter 200 / 283], [train loss 0.31374], [train acc 0.90219]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [val loss 0.21957], [val acc 0.93398]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [iter 100 / 283], [train loss 0.33797], [train acc 0.89187]\n",
      "[epoch 12], [iter 200 / 283], [train loss 0.32208], [train acc 0.89938]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [val loss 0.20086], [val acc 0.94718]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [iter 100 / 283], [train loss 0.29924], [train acc 0.91125]\n",
      "[epoch 13], [iter 200 / 283], [train loss 0.30580], [train acc 0.91156]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [val loss 0.28939], [val acc 0.91021]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [iter 100 / 283], [train loss 0.31894], [train acc 0.89250]\n",
      "[epoch 14], [iter 200 / 283], [train loss 0.32897], [train acc 0.89500]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [val loss 0.23316], [val acc 0.93222]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [iter 100 / 283], [train loss 0.26611], [train acc 0.92312]\n",
      "[epoch 15], [iter 200 / 283], [train loss 0.30072], [train acc 0.90906]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [val loss 0.18634], [val acc 0.95599]\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "          ------------- Fold 2 -----------\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [iter 100 / 283], [train loss 0.78622], [train acc 0.83313]\n",
      "[epoch 1], [iter 200 / 283], [train loss 0.58922], [train acc 0.87344]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 0.76403], [val acc 0.69334]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [iter 100 / 283], [train loss 0.32647], [train acc 0.92750]\n",
      "[epoch 2], [iter 200 / 283], [train loss 0.34599], [train acc 0.91812]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 0.80716], [val acc 0.65341]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [iter 100 / 283], [train loss 0.31560], [train acc 0.91812]\n",
      "[epoch 3], [iter 200 / 283], [train loss 0.33540], [train acc 0.91125]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 0.84195], [val acc 0.66429]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [iter 100 / 283], [train loss 0.27941], [train acc 0.93188]\n",
      "[epoch 4], [iter 200 / 283], [train loss 0.29215], [train acc 0.92406]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 0.86480], [val acc 0.68862]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [iter 100 / 283], [train loss 0.28829], [train acc 0.92250]\n",
      "[epoch 5], [iter 200 / 283], [train loss 0.28963], [train acc 0.92094]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 0.99378], [val acc 0.64509]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [iter 100 / 283], [train loss 0.28505], [train acc 0.92688]\n",
      "[epoch 6], [iter 200 / 283], [train loss 0.27191], [train acc 0.92500]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 0.96353], [val acc 0.66885]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [iter 100 / 283], [train loss 0.26650], [train acc 0.91687]\n",
      "[epoch 7], [iter 200 / 283], [train loss 0.26104], [train acc 0.92219]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 1.00436], [val acc 0.68926]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [iter 100 / 283], [train loss 0.29103], [train acc 0.91375]\n",
      "[epoch 8], [iter 200 / 283], [train loss 0.28344], [train acc 0.91563]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 0.93766], [val acc 0.66773]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [iter 100 / 283], [train loss 0.22425], [train acc 0.94250]\n",
      "[epoch 9], [iter 200 / 283], [train loss 0.22841], [train acc 0.93437]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 0.95695], [val acc 0.66917]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [iter 100 / 283], [train loss 0.23371], [train acc 0.93688]\n",
      "[epoch 10], [iter 200 / 283], [train loss 0.24133], [train acc 0.93344]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 1.19176], [val acc 0.67478]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [iter 100 / 283], [train loss 0.24170], [train acc 0.93500]\n",
      "[epoch 11], [iter 200 / 283], [train loss 0.24418], [train acc 0.93000]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [val loss 1.15102], [val acc 0.65637]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [iter 100 / 283], [train loss 0.23931], [train acc 0.92688]\n",
      "[epoch 12], [iter 200 / 283], [train loss 0.24688], [train acc 0.92594]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [val loss 1.30787], [val acc 0.66245]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [iter 100 / 283], [train loss 0.22861], [train acc 0.93875]\n",
      "[epoch 13], [iter 200 / 283], [train loss 0.22333], [train acc 0.93719]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [val loss 1.22151], [val acc 0.66693]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [iter 100 / 283], [train loss 0.22769], [train acc 0.93063]\n",
      "[epoch 14], [iter 200 / 283], [train loss 0.22057], [train acc 0.93500]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [val loss 1.25905], [val acc 0.66037]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [iter 100 / 283], [train loss 0.22974], [train acc 0.92937]\n",
      "[epoch 15], [iter 200 / 283], [train loss 0.21789], [train acc 0.93563]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [val loss 1.18712], [val acc 0.65741]\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "          ------------- Fold 3 -----------\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [iter 100 / 283], [train loss 0.65392], [train acc 0.87438]\n",
      "[epoch 1], [iter 200 / 283], [train loss 0.50198], [train acc 0.89031]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 0.26033], [val acc 0.92398]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [iter 100 / 283], [train loss 0.28144], [train acc 0.93188]\n",
      "[epoch 2], [iter 200 / 283], [train loss 0.26640], [train acc 0.93312]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 0.38464], [val acc 0.88604]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [iter 100 / 283], [train loss 0.22473], [train acc 0.94750]\n",
      "[epoch 3], [iter 200 / 283], [train loss 0.24575], [train acc 0.93812]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 0.48034], [val acc 0.84203]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [iter 100 / 283], [train loss 0.23488], [train acc 0.93563]\n",
      "[epoch 4], [iter 200 / 283], [train loss 0.22870], [train acc 0.93812]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 0.49619], [val acc 0.86100]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [iter 100 / 283], [train loss 0.19512], [train acc 0.95063]\n",
      "[epoch 5], [iter 200 / 283], [train loss 0.20539], [train acc 0.94531]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 0.65047], [val acc 0.78697]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [iter 100 / 283], [train loss 0.19608], [train acc 0.94500]\n",
      "[epoch 6], [iter 200 / 283], [train loss 0.21245], [train acc 0.94188]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 0.60957], [val acc 0.79449]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [iter 100 / 283], [train loss 0.18801], [train acc 0.95562]\n",
      "[epoch 7], [iter 200 / 283], [train loss 0.20726], [train acc 0.94937]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 1.03251], [val acc 0.66869]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [iter 100 / 283], [train loss 0.17990], [train acc 0.95125]\n",
      "[epoch 8], [iter 200 / 283], [train loss 0.18197], [train acc 0.95188]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 0.48511], [val acc 0.85523]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [iter 100 / 283], [train loss 0.17911], [train acc 0.94750]\n",
      "[epoch 9], [iter 200 / 283], [train loss 0.18182], [train acc 0.94750]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 0.45766], [val acc 0.84603]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [iter 100 / 283], [train loss 0.21548], [train acc 0.94000]\n",
      "[epoch 10], [iter 200 / 283], [train loss 0.21306], [train acc 0.93937]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 0.38992], [val acc 0.88428]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [iter 100 / 283], [train loss 0.17932], [train acc 0.94688]\n",
      "[epoch 11], [iter 200 / 283], [train loss 0.18319], [train acc 0.94906]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [val loss 0.54620], [val acc 0.82851]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [iter 100 / 283], [train loss 0.14571], [train acc 0.96188]\n",
      "[epoch 12], [iter 200 / 283], [train loss 0.17126], [train acc 0.95281]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [val loss 0.35078], [val acc 0.88516]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [iter 100 / 283], [train loss 0.16417], [train acc 0.95250]\n",
      "[epoch 13], [iter 200 / 283], [train loss 0.16339], [train acc 0.95469]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [val loss 0.67647], [val acc 0.79457]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [iter 100 / 283], [train loss 0.13006], [train acc 0.96250]\n",
      "[epoch 14], [iter 200 / 283], [train loss 0.14391], [train acc 0.95719]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [val loss 0.60862], [val acc 0.82626]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [iter 100 / 283], [train loss 0.12486], [train acc 0.96937]\n",
      "[epoch 15], [iter 200 / 283], [train loss 0.14512], [train acc 0.95937]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [val loss 0.49331], [val acc 0.84387]\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "          ------------- Fold 4 -----------\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [iter 100 / 283], [train loss 0.64782], [train acc 0.86625]\n",
      "[epoch 1], [iter 200 / 283], [train loss 0.47290], [train acc 0.90844]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 2.98326], [val acc 0.29209]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [iter 100 / 283], [train loss 0.23596], [train acc 0.95688]\n",
      "[epoch 2], [iter 200 / 283], [train loss 0.23055], [train acc 0.95437]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 2.56623], [val acc 0.21119]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [iter 100 / 283], [train loss 0.18122], [train acc 0.96313]\n",
      "[epoch 3], [iter 200 / 283], [train loss 0.18831], [train acc 0.96125]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 2.59293], [val acc 0.24904]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [iter 100 / 283], [train loss 0.18559], [train acc 0.95813]\n",
      "[epoch 4], [iter 200 / 283], [train loss 0.17896], [train acc 0.95969]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 2.67146], [val acc 0.26136]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [iter 100 / 283], [train loss 0.18942], [train acc 0.96000]\n",
      "[epoch 5], [iter 200 / 283], [train loss 0.17379], [train acc 0.95875]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 2.96719], [val acc 0.20591]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [iter 100 / 283], [train loss 0.16873], [train acc 0.95937]\n",
      "[epoch 6], [iter 200 / 283], [train loss 0.16633], [train acc 0.96062]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 3.20009], [val acc 0.14781]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [iter 100 / 283], [train loss 0.15269], [train acc 0.96313]\n",
      "[epoch 7], [iter 200 / 283], [train loss 0.15333], [train acc 0.96062]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 2.88456], [val acc 0.19046]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [iter 100 / 283], [train loss 0.16377], [train acc 0.95500]\n",
      "[epoch 8], [iter 200 / 283], [train loss 0.15310], [train acc 0.95906]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 3.19921], [val acc 0.16805]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [iter 100 / 283], [train loss 0.13591], [train acc 0.96188]\n",
      "[epoch 9], [iter 200 / 283], [train loss 0.14147], [train acc 0.96344]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 3.26704], [val acc 0.20158]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [iter 100 / 283], [train loss 0.14212], [train acc 0.96500]\n",
      "[epoch 10], [iter 200 / 283], [train loss 0.14044], [train acc 0.96594]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 3.00155], [val acc 0.23143]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [iter 100 / 283], [train loss 0.13491], [train acc 0.96750]\n",
      "[epoch 11], [iter 200 / 283], [train loss 0.13552], [train acc 0.96562]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [val loss 3.44849], [val acc 0.22655]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [iter 100 / 283], [train loss 0.13686], [train acc 0.96875]\n",
      "[epoch 12], [iter 200 / 283], [train loss 0.13425], [train acc 0.96781]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [val loss 3.48819], [val acc 0.15709]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [iter 100 / 283], [train loss 0.12704], [train acc 0.96875]\n",
      "[epoch 13], [iter 200 / 283], [train loss 0.11760], [train acc 0.97031]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [val loss 3.36900], [val acc 0.21255]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [iter 100 / 283], [train loss 0.11252], [train acc 0.97125]\n",
      "[epoch 14], [iter 200 / 283], [train loss 0.11758], [train acc 0.96813]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [val loss 3.52635], [val acc 0.19390]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [iter 100 / 283], [train loss 0.09996], [train acc 0.97688]\n",
      "[epoch 15], [iter 200 / 283], [train loss 0.11579], [train acc 0.96875]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [val loss 3.64800], [val acc 0.17165]\n",
      "------------------------------------------------------------\n",
      "Average Accuracy : 0.7061833119931712\n"
     ]
    }
   ],
   "source": [
    "kfold=KFold(n_splits=5)\n",
    "\n",
    "best_val_acc = 0.88\n",
    "\n",
    "lr = 2e-4 # 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i, (train_index, validate_index) in enumerate(kfold.split(train_data)):\n",
    "    #print(\"train index:\", train_index, \"validate index:\", validate_index)\n",
    "    train_data_i = torch.utils.data.Subset(train_data, train_index)\n",
    "    validation_data_i = torch.utils.data.Subset(train_data, validate_index)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data_i, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
    "    valid_loader = torch.utils.data.DataLoader(validation_data_i, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
    "    \n",
    "    \n",
    "    epoch_num = 15\n",
    "    \n",
    "    model = Additional(resnet_model, num_fits, freeze = False).to(device)\n",
    "    \n",
    "    print('------------------------------------------------------------')\n",
    "    print(f'          ------------- Fold {i} -----------')\n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    total_loss_val, total_acc_val = [],[]\n",
    "    for epoch in range(1, epoch_num+1):\n",
    "        loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
    "        loss_val, acc_val = validate(valid_loader, model, criterion, optimizer, epoch)\n",
    "        total_loss_val.append(loss_val)\n",
    "        total_acc_val.append(acc_val)\n",
    "        \n",
    "        if acc_val > best_val_acc:\n",
    "            best_val_acc = acc_val\n",
    "            torch.save(model.state_dict(), model_name+'freeze_'+str(best_val_acc)[:4]+'.ckpt')\n",
    "            print('*****************************************************')\n",
    "            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "            print('*****************************************************')\n",
    "            \n",
    "    acc.append(acc_val)\n",
    "\n",
    "print(f'Average Accuracy : {np.array(acc).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = X.view(5656, -1)\n",
    "# y1 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX = X1.view(125, 3, 224, -1)\n",
    "# XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX.equal(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before = X1.numpy()\n",
    "# Before.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After = torch.from_numpy(Before)\n",
    "# After.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recons = After.view(5656, 3, 224, -1)\n",
    "# Recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.974939,  1.974939,  1.733488, ..., -1.572877, -0.58127 ,\n",
       "       -0.416003], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.equal(Recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX==Recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, label = SMOTE().fit_sample(X1, y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHxCbEnY_NH7"
   },
   "source": [
    "# Over sample minority class and under sample majority class (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_2 = torch.utils.data.DataLoader(\n",
    "#     train_data, \n",
    "#     sampler=ImbalancedDatasetSampler(train_data),\n",
    "#     batch_size=args.batch_size, \n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.combine import SMOTETomek # doctest: +NORMALIZE_WHITESPACE\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, y_ = make_classification(n_classes=4, class_sep=2,\n",
    "... weights=[0.2, 0.1, 0.3, 0.5], n_informative=3, n_redundant=1, flip_y=0,\n",
    "... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({3: 400, 2: 300, 0: 200, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smt = SMOTETomek(random_state=42)\n",
    "# smt = Smote()\n",
    "smt = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res_, y_res_ = smt.fit_resample(X_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({2: 400, 0: 400, 1: 400, 3: 400})\n"
     ]
    }
   ],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_res_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# class Smote:\n",
    "#     \"\"\"\n",
    "#     SMOTE过采样算法.\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     k: int\n",
    "#         选取的近邻数目.\n",
    "#     sampling_rate: int\n",
    "#         采样倍数, attention sampling_rate < k.\n",
    "#     newindex: int\n",
    "#         生成的新样本(合成样本)的索引号.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, sampling_rate=5, k=5):\n",
    "#         self.sampling_rate = sampling_rate\n",
    "#         self.k = k\n",
    "#         self.newindex = 0\n",
    " \n",
    "#     def fit(self, X, y=None):\n",
    "#         if y is not None:\n",
    "#             negative_X = X[y == 0]\n",
    "#             X = X[y == 1]\n",
    " \n",
    "#         n_samples, n_features = X.shape\n",
    "#         # 初始化一个矩阵, 用来存储合成样本\n",
    "#         self.synthetic = np.zeros((n_samples * self.sampling_rate, n_features))\n",
    " \n",
    "#         # 找出正样本集(数据集X)中的每一个样本在数据集X中的k个近邻\n",
    "#         knn = NearestNeighbors(n_neighbors=self.k).fit(X)\n",
    "#         for i in range(len(X)):\n",
    "#             k_neighbors = knn.kneighbors(X[i].reshape(1, -1),return_distance=False)[0]\n",
    "#             # 对正样本集(minority class samples)中每个样本, 分别根据其k个近邻生成\n",
    "#             # sampling_rate个新的样本\n",
    "#             self.synthetic_samples(X, i, k_neighbors)\n",
    " \n",
    "#         if y is not None:\n",
    "#             return (np.concatenate((self.synthetic, X, negative_X), axis=0),\n",
    "#                     np.concatenate(([1] * (len(self.synthetic) + len(X)), y[y == 0]), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SMOTE' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-77d8a3df4433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_res_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'SMOTE' object is not iterable"
     ]
    }
   ],
   "source": [
    "X_res_, y_res_ = smt.fit(X_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLKDt9r38dGZ"
   },
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# valid_sampler = SubsetRandomSampler(val_indices)\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "#-----------------------------------------------------------------------------z\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "ipdb.set_trace()\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "# get target\n",
    "\n",
    "targets=[]\n",
    "#data=[]\n",
    "\n",
    "for i in train_data.file_list:\n",
    "    targets.append(i[0])\n",
    "   # data.append(i[2])\n",
    "    \n",
    "target_train=targets[split:]\n",
    "train_set=data[split:]\n",
    "print(len(train_set))\n",
    "\n",
    "target_test=targets[:split]\n",
    "#test_set=data[:split]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# for i in test_data.file_list:\n",
    "#     test_targets.append(i[0])\n",
    "    \n",
    "# target = torch.cat((torch.zeros(int(len(train_data) * 0.99), dtype=torch.long),\n",
    "#                     torch.ones(int(len(train_data) * 0.01), dtype=torch.long)))\n",
    "\n",
    "\n",
    "#count classes\n",
    "class_count = np.unique(target_train ,return_counts=True)[1]\n",
    "#print(class_count)\n",
    "class_count_test= np.unique(target_test, return_counts=True)[1]\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# get weights train\n",
    "\n",
    "weight_train = 1. / class_count\n",
    "#print(targets)\n",
    "samples_weight_train = weight_train[target_train]\n",
    "#print(samples_weight_train )\n",
    "\n",
    "samples_weight_train = torch.from_numpy(samples_weight_train)\n",
    "#print(samples_weight_train)\n",
    "sampler_train = WeightedRandomSampler(samples_weight_train, len(samples_weight_train))\n",
    "\n",
    "#### valid\n",
    "\n",
    "\n",
    "\n",
    "weight_test = 1. / class_count_test\n",
    "#print(targets)\n",
    "samples_weight_test = weight_test[target_test]\n",
    "#print(samples_weight_train )\n",
    "\n",
    "samples_weight_test = torch.from_numpy(samples_weight_test)\n",
    "#print(samples_weight_train)\n",
    "sampler_valid = WeightedRandomSampler(samples_weight_test, len(samples_weight_test))\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "                                             sampler=sampler_train)\n",
    "\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "                                             sampler=sampler_valid)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "#                                              sampler=valid_sampler)\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkQefu0a8sz2"
   },
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3qezIkE85cj"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,input,n_class , dropout=0.2):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.conv=nn.Sequential(nn.Conv2d(input,32,5),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.MaxPool2d(3),\n",
    "                                nn.Conv2d(32,64,3),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(64),\n",
    "                                nn.MaxPool2d(3),\n",
    "                                nn.Conv2d(64,128,3),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(128),\n",
    "                                nn.MaxPool2d(3)\n",
    "                               )\n",
    "        \n",
    "        self.fc1=nn.Linear(7*7*128,512)\n",
    "        self.dropout =nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2=nn.Linear(512,1024)\n",
    "        \n",
    "        self.fc3=nn.Linear(1024,256)\n",
    "        \n",
    "        self.fc4=nn.Linear(256,n_class)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)        \n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.fc1(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.fc2(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.fc3(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.fc4(x)        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZR2wwkh86yK"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jz-VT3wy9NT5"
   },
   "source": [
    " # Model whith cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOocz4mU9OCw"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model(3,5)\n",
    "model=model.to(device)\n",
    "lr = 1e-4  # 0.001\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "train_loader.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "x = torch.Tensor(class_weights_normalized)\n",
    "x = x.to(device)\n",
    "x = x\n",
    "x,class_distrbution\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs=30\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    n=0\n",
    "    loss_list = []\n",
    "    acc_list = []   \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "#         correct=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Run the forward pass\n",
    "        images, labels = data\n",
    "        #print(images.size())\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)   \n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        #print(total)\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        \n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/total)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "\n",
    "            \n",
    "#######################################################################################\n",
    "\n",
    "model.eval()\n",
    "val_loss = AverageMeter()\n",
    "val_acc = AverageMeter()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKBoSrKB9u6w"
   },
   "source": [
    "#  Model with focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr2q2fuQ9pS5"
   },
   "outputs": [],
   "source": [
    "def focalLoss(outputs, targets, w, alpha=1, gamma=4):\n",
    "    ce_loss = torch.nn.functional.cross_entropy(outputs, targets, reduction='none') # important to add reduction='none' to keep per-batch-item loss\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = (alpha * (1-pt)**gamma * ce_loss).mean() # mean over the batch\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight=None, \n",
    "                 gamma=2., reduction='none'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob, \n",
    "            target_tensor, \n",
    "            weight=self.weight,\n",
    "            reduction = self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thAAvd6q937a"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#--------------------------------------------------------------------------------------#\n",
    "train_loader.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "x = torch.Tensor(class_weights_normalized)\n",
    "x = x.to(device)\n",
    "x = x\n",
    "x,class_distrbution\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "\n",
    "model = Model(3,5)\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    "lr = 1e-4   # 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs=30\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "\n",
    "    model.train()\n",
    "    n=0\n",
    "    loss_list = []\n",
    "    acc_list = []  \n",
    "     \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "#         correct=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Run the forward pass\n",
    "        images, labels = data\n",
    "        #print(images.size())\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)   \n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.size())\n",
    "        loss =  focalLoss(outputs, labels,x)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        #print(total)\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        \n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/total)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "\n",
    " model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(focalLoss(outputs, labels,x).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "from dataset import CassavaDataset #,CellsDataset, PatchedDataset\n",
    "from model import Model#, UNet\n",
    "# from utils import plot_cells, plot_masks, plot_mask_cells\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WeightedRandomSampler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
