{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "d3cVbqIs7PDi",
    "outputId": "19f10d20-d20f-4a31-d1d4-b97da869c7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0+cu101\n",
      "Torchvision Version:  0.6.0+cu101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "#import pretrainedmodels\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB5I0Rw47gBn"
   },
   "outputs": [],
   "source": [
    "# print('Train set:')\n",
    "# for cls in os.listdir(\"../input/ammi-2020-convnets/train/train\"):\n",
    "#     print('{}:{}'.format(cls, len(os.listdir(os.path.join(\"../input/ammi-2020-convnets/train/train\", cls)))))\n",
    "# im = Image.open('../input/ammi-2020-convnets/train/train/cgm/train-cgm-738.jpg')\n",
    "# print(im.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60SznP4x7qZb"
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/train/train\"\n",
    "test_path = \"./data/test/test\"\n",
    "extraimage_path = \"./data/extraimages/extraimages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "cbb:466\n",
      "healthy:316\n",
      "cbsd:1443\n",
      "cmd:2658\n",
      "cgm:773\n",
      "(500, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cbb': 466, 'cbsd': 1443, 'cgm': 773, 'cmd': 2658, 'healthy': 316}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train set:')\n",
    "class_distrbution = {}\n",
    "for cls in os.listdir(data_path):\n",
    "    print('{}:{}'.format(cls, len(os.listdir(os.path.join(data_path, cls)))))\n",
    "    class_distrbution[cls] =  len(os.listdir(os.path.join(data_path, cls)))\n",
    "im = Image.open(data_path+'/cgm/train-cgm-738.jpg')\n",
    "print(im.size)\n",
    "class_distrbution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq7aPxZO8Nyc"
   },
   "source": [
    "# Distribution of the classes in the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BKbVY308Ss2"
   },
   "outputs": [],
   "source": [
    "# Transformations for both the training and testing data\n",
    "\n",
    "# Transformations for both the training and testing data\n",
    "mean=[0.4543, 0.5137, 0.3240]\n",
    "std=[0.1949, 0.1977, 0.1661]\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224), #448, 299, 224, 331\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean,std=std)])\n",
    "\n",
    "test_transforms = transforms.Compose([ transforms.Resize(224),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean,std=std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9Za0HGZ8W4F"
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.classes = os.listdir(path)\n",
    "        self.path = [f\"{path}/{className}\" for className in self.classes]\n",
    "        self.file_list = [glob.glob(f\"{x}/*\") for x in self.path]\n",
    "        self.transform = transform\n",
    "\n",
    "        files = []\n",
    "        class_names = {}\n",
    "        for i, className in enumerate(self.classes):\n",
    "            for fileName in self.file_list[i]:\n",
    "                files.append([i, className, fileName])\n",
    "\n",
    "                name = str(i)+'-'+className\n",
    "                if name not in class_names:\n",
    "                    class_names[name] = 1\n",
    "                else:\n",
    "                    class_names[name] += 1\n",
    "        self.file_list = files\n",
    "#         print(class_names)\n",
    "        files = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fileName = self.file_list[idx][2]\n",
    "        classCategory = self.file_list[idx][0]\n",
    "        im = Image.open(fileName)\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "# #         return im.view(3, 448, 448), classCategory\n",
    "        return im.view(3, 224, 224), classCategory\n",
    "# #         return im.view(3, 299, 299), classCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0jldgqi8aqT"
   },
   "outputs": [],
   "source": [
    "train_data = CassavaDataset(data_path, transform=train_transforms)\n",
    "\n",
    "test_data = CassavaDataset(test_path, transform=test_transforms)\n",
    "\n",
    "extraimage_data = CassavaDataset(extraimage_path, transform=train_transforms) #maybe need an other trasforms, I had to change the dataset structure :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "validation_split = 0.0\n",
    "shuffle_dataset = True\n",
    "random_seed= 42 #42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "batch_size = 125 #16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                             sampler=train_sampler)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "#                                              sampler=valid_sampler)\n",
    "\n",
    "unlabeled_loader = torch.utils.data.DataLoader(extraimage_data, batch_size=batch_size) # to make batch_size work, I had to moove all the unlabeled data in a 0 folder\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1) # make batch = 1 here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_train = 0\n",
    "# from collections import Counter\n",
    "\n",
    "# Classes = Counter()\n",
    "\n",
    "# for i, (data, label) in enumerate(train_loader):\n",
    "#     total_train += i\n",
    "#     Classes.update(label.tolist())# += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5656"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "np.set_printoptions(precision=6)\n",
    "# torch.set_printoptions(precision=6)\n",
    "torch.set_printoptions(precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_2d_numpy(dataset):\n",
    "    np.set_printoptions(precision=6)\n",
    "    torch.set_printoptions(precision=6)\n",
    "    \n",
    "    batch_size = dataset.__len__() # to take the full data\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    \n",
    "    for (X_hat, y_hat) in train_loader:\n",
    "        X = X_hat.view(batch_size, -1)\n",
    "        X = X.numpy()\n",
    "        y = y_hat.numpy()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tensor_to_2d_numpy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656, 150528)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.combine import SMOTETomek # doctest: +NORMALIZE_WHITESPACE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({3: 2658, 2: 1443, 4: 773, 0: 466, 1: 316})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656, 150528)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = smt.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_2d_numpy_back_to_tensor(X, y, img_size = 224):\n",
    "    np.set_printoptions(precision=6)\n",
    "    torch.set_printoptions(precision=6)\n",
    "    \n",
    "    X_hat = torch.from_numpy(X)\n",
    "    \n",
    "    y_hat = torch.from_numpy(y)\n",
    "    \n",
    "    X_hat = X_hat.view(X[0], 3, 224, -1)\n",
    "\n",
    "#     batch_size = dataset.__len__() # to take the full data\n",
    "    \n",
    "#     train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    \n",
    "#     for (X_hat, y_hat) in train_loader:\n",
    "#         X = X_hat.view(batch_size, -1)\n",
    "#         X = X.numpy()\n",
    "#         y = y_hat.numpy()\n",
    "    return X_hat, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(my_y)\n",
    "\n",
    "my_dataset = data.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "my_dataloader = data.DataLoader(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = next(train_loader.__iter__())[0]\n",
    "# y = next(train_loader.__iter__(a))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = X.view(5656, -1)\n",
    "# y1 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX = X1.view(125, 3, 224, -1)\n",
    "# XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX.equal(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before = X1.numpy()\n",
    "# Before.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After = torch.from_numpy(Before)\n",
    "# After.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recons = After.view(5656, 3, 224, -1)\n",
    "# Recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.974939,  1.974939,  1.733488, ..., -1.572877, -0.58127 ,\n",
       "       -0.416003], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.equal(Recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX==Recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, label = SMOTE().fit_sample(X1, y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHxCbEnY_NH7"
   },
   "source": [
    "# Over sample minority class and under sample majority class (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_2 = torch.utils.data.DataLoader(\n",
    "#     train_data, \n",
    "#     sampler=ImbalancedDatasetSampler(train_data),\n",
    "#     batch_size=args.batch_size, \n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.combine import SMOTETomek # doctest: +NORMALIZE_WHITESPACE\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, y_ = make_classification(n_classes=4, class_sep=2,\n",
    "... weights=[0.2, 0.1, 0.3, 0.5], n_informative=3, n_redundant=1, flip_y=0,\n",
    "... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({3: 400, 2: 300, 0: 200, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smt = SMOTETomek(random_state=42)\n",
    "# smt = Smote()\n",
    "smt = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res_, y_res_ = smt.fit_resample(X_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({2: 400, 0: 400, 1: 400, 3: 400})\n"
     ]
    }
   ],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_res_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# class Smote:\n",
    "#     \"\"\"\n",
    "#     SMOTE过采样算法.\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     k: int\n",
    "#         选取的近邻数目.\n",
    "#     sampling_rate: int\n",
    "#         采样倍数, attention sampling_rate < k.\n",
    "#     newindex: int\n",
    "#         生成的新样本(合成样本)的索引号.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, sampling_rate=5, k=5):\n",
    "#         self.sampling_rate = sampling_rate\n",
    "#         self.k = k\n",
    "#         self.newindex = 0\n",
    " \n",
    "#     def fit(self, X, y=None):\n",
    "#         if y is not None:\n",
    "#             negative_X = X[y == 0]\n",
    "#             X = X[y == 1]\n",
    " \n",
    "#         n_samples, n_features = X.shape\n",
    "#         # 初始化一个矩阵, 用来存储合成样本\n",
    "#         self.synthetic = np.zeros((n_samples * self.sampling_rate, n_features))\n",
    " \n",
    "#         # 找出正样本集(数据集X)中的每一个样本在数据集X中的k个近邻\n",
    "#         knn = NearestNeighbors(n_neighbors=self.k).fit(X)\n",
    "#         for i in range(len(X)):\n",
    "#             k_neighbors = knn.kneighbors(X[i].reshape(1, -1),return_distance=False)[0]\n",
    "#             # 对正样本集(minority class samples)中每个样本, 分别根据其k个近邻生成\n",
    "#             # sampling_rate个新的样本\n",
    "#             self.synthetic_samples(X, i, k_neighbors)\n",
    " \n",
    "#         if y is not None:\n",
    "#             return (np.concatenate((self.synthetic, X, negative_X), axis=0),\n",
    "#                     np.concatenate(([1] * (len(self.synthetic) + len(X)), y[y == 0]), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SMOTE' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-77d8a3df4433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_res_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'SMOTE' object is not iterable"
     ]
    }
   ],
   "source": [
    "X_res_, y_res_ = smt.fit(X_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLKDt9r38dGZ"
   },
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# valid_sampler = SubsetRandomSampler(val_indices)\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "#-----------------------------------------------------------------------------z\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "ipdb.set_trace()\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "# get target\n",
    "\n",
    "targets=[]\n",
    "#data=[]\n",
    "\n",
    "for i in train_data.file_list:\n",
    "    targets.append(i[0])\n",
    "   # data.append(i[2])\n",
    "    \n",
    "target_train=targets[split:]\n",
    "train_set=data[split:]\n",
    "print(len(train_set))\n",
    "\n",
    "target_test=targets[:split]\n",
    "#test_set=data[:split]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# for i in test_data.file_list:\n",
    "#     test_targets.append(i[0])\n",
    "    \n",
    "# target = torch.cat((torch.zeros(int(len(train_data) * 0.99), dtype=torch.long),\n",
    "#                     torch.ones(int(len(train_data) * 0.01), dtype=torch.long)))\n",
    "\n",
    "\n",
    "#count classes\n",
    "class_count = np.unique(target_train ,return_counts=True)[1]\n",
    "#print(class_count)\n",
    "class_count_test= np.unique(target_test, return_counts=True)[1]\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# get weights train\n",
    "\n",
    "weight_train = 1. / class_count\n",
    "#print(targets)\n",
    "samples_weight_train = weight_train[target_train]\n",
    "#print(samples_weight_train )\n",
    "\n",
    "samples_weight_train = torch.from_numpy(samples_weight_train)\n",
    "#print(samples_weight_train)\n",
    "sampler_train = WeightedRandomSampler(samples_weight_train, len(samples_weight_train))\n",
    "\n",
    "#### valid\n",
    "\n",
    "\n",
    "\n",
    "weight_test = 1. / class_count_test\n",
    "#print(targets)\n",
    "samples_weight_test = weight_test[target_test]\n",
    "#print(samples_weight_train )\n",
    "\n",
    "samples_weight_test = torch.from_numpy(samples_weight_test)\n",
    "#print(samples_weight_train)\n",
    "sampler_valid = WeightedRandomSampler(samples_weight_test, len(samples_weight_test))\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "                                             sampler=sampler_train)\n",
    "\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "                                             sampler=sampler_valid)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "#                                              sampler=valid_sampler)\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkQefu0a8sz2"
   },
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3qezIkE85cj"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,input,n_class , dropout=0.2):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.conv=nn.Sequential(nn.Conv2d(input,32,5),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.MaxPool2d(3),\n",
    "                                nn.Conv2d(32,64,3),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(64),\n",
    "                                nn.MaxPool2d(3),\n",
    "                                nn.Conv2d(64,128,3),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(128),\n",
    "                                nn.MaxPool2d(3)\n",
    "                               )\n",
    "        \n",
    "        self.fc1=nn.Linear(7*7*128,512)\n",
    "        self.dropout =nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2=nn.Linear(512,1024)\n",
    "        \n",
    "        self.fc3=nn.Linear(1024,256)\n",
    "        \n",
    "        self.fc4=nn.Linear(256,n_class)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)        \n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.fc1(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.fc2(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.fc3(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.fc4(x)        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZR2wwkh86yK"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jz-VT3wy9NT5"
   },
   "source": [
    " # Model whith cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOocz4mU9OCw"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model(3,5)\n",
    "model=model.to(device)\n",
    "lr = 1e-4  # 0.001\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "train_loader.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "x = torch.Tensor(class_weights_normalized)\n",
    "x = x.to(device)\n",
    "x = x\n",
    "x,class_distrbution\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs=30\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    n=0\n",
    "    loss_list = []\n",
    "    acc_list = []   \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "#         correct=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Run the forward pass\n",
    "        images, labels = data\n",
    "        #print(images.size())\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)   \n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        #print(total)\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        \n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/total)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "\n",
    "            \n",
    "#######################################################################################\n",
    "\n",
    "model.eval()\n",
    "val_loss = AverageMeter()\n",
    "val_acc = AverageMeter()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKBoSrKB9u6w"
   },
   "source": [
    "#  Model with focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr2q2fuQ9pS5"
   },
   "outputs": [],
   "source": [
    "def focalLoss(outputs, targets, w, alpha=1, gamma=4):\n",
    "    ce_loss = torch.nn.functional.cross_entropy(outputs, targets, reduction='none') # important to add reduction='none' to keep per-batch-item loss\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = (alpha * (1-pt)**gamma * ce_loss).mean() # mean over the batch\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight=None, \n",
    "                 gamma=2., reduction='none'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob, \n",
    "            target_tensor, \n",
    "            weight=self.weight,\n",
    "            reduction = self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thAAvd6q937a"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#--------------------------------------------------------------------------------------#\n",
    "train_loader.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "x = torch.Tensor(class_weights_normalized)\n",
    "x = x.to(device)\n",
    "x = x\n",
    "x,class_distrbution\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "\n",
    "model = Model(3,5)\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    "lr = 1e-4   # 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs=30\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "\n",
    "    model.train()\n",
    "    n=0\n",
    "    loss_list = []\n",
    "    acc_list = []  \n",
    "     \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "#         correct=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Run the forward pass\n",
    "        images, labels = data\n",
    "        #print(images.size())\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)   \n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.size())\n",
    "        loss =  focalLoss(outputs, labels,x)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        #print(total)\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        \n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/total)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "\n",
    " model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(focalLoss(outputs, labels,x).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "from dataset import CassavaDataset #,CellsDataset, PatchedDataset\n",
    "from model import Model#, UNet\n",
    "# from utils import plot_cells, plot_masks, plot_mask_cells\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WeightedRandomSampler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
