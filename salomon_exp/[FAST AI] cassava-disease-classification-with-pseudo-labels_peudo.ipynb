{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pretrainedmodels in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (0.7.4)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (from pretrainedmodels) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (from pretrainedmodels) (4.45.0)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (from pretrainedmodels) (0.5.0)\n",
      "Requirement already satisfied: munch in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (from pretrainedmodels) (2.5.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (1.16.5)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ubuntu/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (5.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import fastai\n",
    "from fastai import vision\n",
    "from fastai.vision import *\n",
    "\n",
    "\n",
    "import pretrainedmodels as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda config --set ssl_verify no && conda install -y -c pytorch pytorch=1.4.0 torchvision cuda92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.4.0\n",
    "# !pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pseudo-Labels for the Extra Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_kwargs = {\"do_flip\": True,\n",
    "                    \"flip_vert\": True,\n",
    "                    \"max_rotate\": 180,\n",
    "                    \"max_zoom\": 1.1,\n",
    "                    \"max_lighting\": 0.2,\n",
    "                    \"max_warp\": 0.2,\n",
    "                    \"p_affine\": 0.75,\n",
    "                    \"p_lighting\": 0.7}\n",
    "        \n",
    "transforms = vision.get_transforms(**transform_kwargs)\n",
    "\n",
    "data_bunch_kwargs = {\"path\": \"./data/train\",\n",
    "                     \"train\": \"train\",\n",
    "                     \"valid_pct\": 0.1,\n",
    "                     \"size\": 448,\n",
    "                     \"bs\": 16,\n",
    "                     \"ds_tfms\": transforms,\n",
    "                     \"test\": \"./data/extraimages/extraimages\"}\n",
    "\n",
    "get_transforms()\n",
    "\n",
    "image_data_bunch = (vision.ImageDataBunch \n",
    "                          .from_folder(**data_bunch_kwargs)\n",
    "                          .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './models/3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8e5fdb7006ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./models/3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './models/3'"
     ]
    }
   ],
   "source": [
    "shutil.copytree(\"./models/\", \"./models/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our pretrained model to predict the labels for our extra images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Additional(nn.Module):\n",
    "    def __init__(self, modelA,in_features,nb_classes=5, freeze = False):\n",
    "        super(Additional, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        # Remove last linear layer\n",
    "#         self.modelA.fc = nn.Identity() # for resnet\n",
    "        self.modelA.last_linear = nn.Identity() #for re_renext\n",
    "#         self.modelA.classifier = nn.Identity()    # densenet201\n",
    "        for p in self.modelA.parameters():\n",
    "            if freeze:\n",
    "                p.requires_grad = False\n",
    "            else :\n",
    "                p.requires_grad = True\n",
    "        \n",
    "        # Create new classifier\n",
    "        self.fc_1 = nn.Linear(in_features,256)\n",
    "        self.fc_2 = nn.Linear(256,  512)\n",
    "        self.fc_out = nn.Linear( 512, nb_classes)\n",
    "        \n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #model\n",
    "        x = self.modelA(x.clone())  \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #FC\n",
    "        x  = self.dropout(self.fc_1(F.relu(x)))\n",
    "        x = self.dropout(self.fc_2(F.relu(x)))\n",
    "        x = self.fc_out(F.relu(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'se_resnext101_32x4d' # se_resnext101_32x4d, resnext101_64x4d, nasnetlarge\n",
    "# resnet_model = torch.hub.load('pytorch/vision:v0.5.0', model_name, pretrained=True)\n",
    "resnet_model = pm.se_resnext101_32x4d(num_classes=1000, pretrained=\"imagenet\")# todo : how to pretrained=False ?\n",
    "\n",
    "# resnet_model = torchvision.models.resnet50(pretrained=True)\n",
    "#---------------------------------------------\n",
    "\n",
    "# num_fits = resnet_model.fc.in_features\n",
    "num_fits = resnet_model.last_linear.in_features # se_resnext101_32x4d\n",
    "# num_fits = resnet_model.classifier.in_features # densenet201\n",
    "num_fits\n",
    "\n",
    "\n",
    "model = Additional(resnet_model, num_fits, freeze = False)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No weight layer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-537656fe3859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m_base_arch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data_bunch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_base_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"se_resnext101_32x4d0.88\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages/fastai/vision/learner.py\u001b[0m in \u001b[0;36mcnn_learner\u001b[0;34m(data, base_arch, cut, pretrained, lin_ftrs, ps, custom_head, split_on, bn_final, init, concat_pool, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     model = create_cnn_model(base_arch, data.c, cut, pretrained, lin_ftrs, ps=ps, custom_head=custom_head,\n\u001b[0;32m---> 98\u001b[0;31m         bn_final=bn_final, concat_pool=concat_pool)\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_on\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages/fastai/vision/learner.py\u001b[0m in \u001b[0;36mcreate_cnn_model\u001b[0;34m(base_arch, nc, cut, pretrained, lin_ftrs, ps, custom_head, bn_final, concat_pool)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_head\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_features_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconcat_pool\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages/fastai/callbacks/hooks.py\u001b[0m in \u001b[0;36mnum_features_model\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0msz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0msz\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages/fastai/callbacks/hooks.py\u001b[0m in \u001b[0;36mmodel_sizes\u001b[0;34m(m, size)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;34m\"Pass a dummy input through the model `m` to get the various sizes of activations.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mhook_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages/fastai/callbacks/hooks.py\u001b[0m in \u001b[0;36mdummy_eval\u001b[0;34m(m, size)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdummy_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m\"Pass a `dummy_batch` in evaluation mode in `m` with `size`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mHooks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages/fastai/callbacks/hooks.py\u001b[0m in \u001b[0;36mdummy_batch\u001b[0;34m(m, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdummy_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"Create a dummy batch to go through `m` with `size`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mch_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mone_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws_neuron_mxnet_p36/lib/python3.6/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36min_channels\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflatten_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No weight layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mModelOnCPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No weight layer"
     ]
    }
   ],
   "source": [
    "_base_arch = lambda arg: model\n",
    "learner = vision.cnn_learner(image_data_bunch, base_arch=_base_arch, pretrained=False, metrics=vision.error_rate, model_dir=\"./models\")\n",
    "_ = learner.load(\"se_resnext101_32x4d0.88\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='8', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      37.50% [3/8 20:03<33:25]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='301' class='' max='788', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      38.20% [301/788 02:32<04:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_probabilities, _ = learner.TTA(ds_type=fastai.basic_data.DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_probabilities, _predicted_classes = predicted_probabilities.max(dim=1)\n",
    "class_labels = np.array(['cbb','cbsd','cgm','cmd','healthy'])\n",
    "predicted_class_labels = class_labels[_predicted_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new training data set\n",
    "\n",
    "Once we have the pseudo-labels and predicted probabilities that these pseudo-labels are correct we need to decide which of the extra images we want to include in our new, larger training data set. Basic idea is to choose a decision threshold and then only include the extra images where our original model was highly confident in the predicted pseduo-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copytree(\"../input/cassava-disease/train/train/\", \"./data/train\")\n",
    "shutil.copytree(\"../input/cassava-disease/test/test/\", \"./data/test\")\n",
    "\n",
    "threshold = 0.95  # only include pseudo-labeled images where model is sufficiently confident in its prediction\n",
    "filenames = [item.name for item in learner.data.test_ds.items]\n",
    "for predicted_class_label, predicted_class_probability, filename in zip(predicted_class_labels, predicted_class_probabilities, filenames):\n",
    "    if predicted_class_probability > threshold:\n",
    "        shutil.copy(f\"../input/cassava-disease/extraimages/extraimages/{filename}\", f\"./data/train/{predicted_class_label}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_kwargs = {\"do_flip\": True,\n",
    "                    \"flip_vert\": True,\n",
    "                    \"max_rotate\": 180,\n",
    "                    \"max_zoom\": 1.1,\n",
    "                    \"max_lighting\": 0.2,\n",
    "                    \"max_warp\": 0.2,\n",
    "                    \"p_affine\": 0.75,\n",
    "                    \"p_lighting\": 0.7}\n",
    "        \n",
    "transforms = vision.get_transforms(**transform_kwargs)\n",
    "\n",
    "data_bunch_kwargs = {\"path\": \"./data/train\",\n",
    "                     \"train\": \"train\",\n",
    "                     \"valid_pct\": 0.1,\n",
    "                     \"size\": 448,\n",
    "                     \"bs\": 16,\n",
    "                     \"ds_tfms\": transforms,\n",
    "                     \"test\": \"../test\"}\n",
    "\n",
    "image_data_bunch = (vision.ImageDataBunch\n",
    "                          .from_folder(**data_bunch_kwargs)\n",
    "                          .normalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on new training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_base_arch = lambda arg: pm.se_resnext101_32x4d(num_classes=5, pretrained=None)\n",
    "learner = vision.cnn_learner(image_data_bunch, base_arch=_base_arch, pretrained=False, metrics=vision.error_rate, model_dir=\"/kaggle/working/models/se-resnext101-32x4d\")\n",
    "_ = learner.load(\"best-model-stage-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(learner.recorder\n",
    "        .plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='15', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      53.33% [8/15 57:53<50:39]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.322894</td>\n",
       "      <td>0.147282</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>07:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.317687</td>\n",
       "      <td>0.135908</td>\n",
       "      <td>0.044471</td>\n",
       "      <td>07:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318753</td>\n",
       "      <td>0.128661</td>\n",
       "      <td>0.039663</td>\n",
       "      <td>07:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.341035</td>\n",
       "      <td>0.128646</td>\n",
       "      <td>0.043269</td>\n",
       "      <td>07:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.277657</td>\n",
       "      <td>0.136997</td>\n",
       "      <td>0.045673</td>\n",
       "      <td>07:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.336265</td>\n",
       "      <td>0.122999</td>\n",
       "      <td>0.042067</td>\n",
       "      <td>07:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.297175</td>\n",
       "      <td>0.129257</td>\n",
       "      <td>0.044471</td>\n",
       "      <td>07:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.294812</td>\n",
       "      <td>0.134234</td>\n",
       "      <td>0.050481</td>\n",
       "      <td>07:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='200' class='' max='468', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      42.74% [200/468 02:58<03:59 0.3218]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.1472819447517395.\n",
      "Better model found at epoch 1 with valid_loss value: 0.13590756058692932.\n",
      "Better model found at epoch 2 with valid_loss value: 0.1286608725786209.\n",
      "Better model found at epoch 3 with valid_loss value: 0.12864600121974945.\n",
      "Better model found at epoch 5 with valid_loss value: 0.12299854308366776.\n"
     ]
    }
   ],
   "source": [
    "_save_model_kwargs = {\"every\": \"improvement\",\n",
    "                      \"monitor\": \"valid_loss\",\n",
    "                      \"name\": \"best-model-stage-3\"}\n",
    "_save_model = (fastai.callbacks\n",
    "                     .SaveModelCallback(learner, **_save_model_kwargs))\n",
    "learner.fit_one_cycle(15, max_lr=slice(None, 1e-6, None), callbacks=[_save_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions using TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_probabilities, _ = learner.TTA(ds_type=fastai.basic_data.DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _predicted_classes = predicted_probabilities.max(dim=1)\n",
    "_predicted_class_labels = class_labels[_predicted_classes]\n",
    "\n",
    "_filenames = np.array([item.name for item in image_data_bunch.test_ds.items])\n",
    "\n",
    "submission = (pd.DataFrame\n",
    "                .from_dict({'Category': _predicted_class_labels,'Id': _filenames}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission-using-pseudo-labels.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./data\") # remove unnecessary output files!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aws_neuron_mxnet_p36]",
   "language": "python",
   "name": "conda-env-aws_neuron_mxnet_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
