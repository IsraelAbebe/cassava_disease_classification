{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "d3cVbqIs7PDi",
    "outputId": "19f10d20-d20f-4a31-d1d4-b97da869c7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0+cu101\n",
      "Torchvision Version:  0.6.0+cu101\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "#import pretrainedmodels\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB5I0Rw47gBn"
   },
   "outputs": [],
   "source": [
    "print('Train set:')\n",
    "for cls in os.listdir(\"../input/ammi-2020-convnets/train/train\"):\n",
    "    print('{}:{}'.format(cls, len(os.listdir(os.path.join(\"../input/ammi-2020-convnets/train/train\", cls)))))\n",
    "im = Image.open('../input/ammi-2020-convnets/train/train/cgm/train-cgm-738.jpg')\n",
    "print(im.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60SznP4x7qZb"
   },
   "outputs": [],
   "source": [
    "data_path = \"../input/ammi-2020-convnets/train/train\"\n",
    "test_path = \"../input/ammi-2020-convnets/test/test\"\n",
    "extraimage_path = \"../input/ammi-2020-convnets/extraimages/extraimages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq7aPxZO8Nyc"
   },
   "source": [
    "# Distribution of the classes in the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3O8X8sE8Puq"
   },
   "outputs": [],
   "source": [
    "print('Train set:')\n",
    "class_distrbution = {}\n",
    "for cls in os.listdir(data_path):\n",
    "    print('{}:{}'.format(cls, len(os.listdir(os.path.join(data_path, cls)))))\n",
    "    class_distrbution[cls] =  len(os.listdir(os.path.join(data_path, cls)))\n",
    "im = Image.open(data_path+'/cgm/train-cgm-738.jpg')\n",
    "print(im.size)\n",
    "class_distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BKbVY308Ss2"
   },
   "outputs": [],
   "source": [
    "# Transformations for both the training and testing data\n",
    "\n",
    "# Transformations for both the training and testing data\n",
    "mean=[0.4543, 0.5137, 0.3240]\n",
    "std=[0.1949, 0.1977, 0.1661]\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224), #448, 299, 224, 331\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean,std=std)])\n",
    "\n",
    "test_transforms = transforms.Compose([ transforms.Resize(224),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean,std=std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9Za0HGZ8W4F"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.classes = os.listdir(path)\n",
    "        self.path = [f\"{path}/{className}\" for className in self.classes]\n",
    "        self.file_list = [glob.glob(f\"{x}/*\") for x in self.path]\n",
    "        self.transform = transform\n",
    "\n",
    "        files = []\n",
    "        class_names = {}\n",
    "        for i, className in enumerate(self.classes):\n",
    "            for fileName in self.file_list[i]:\n",
    "                files.append([i, className, fileName])\n",
    "\n",
    "                name = str(i)+'-'+className\n",
    "                if name not in class_names:\n",
    "                    class_names[name] = 1\n",
    "                else:\n",
    "                    class_names[name] += 1\n",
    "        self.file_list = files\n",
    "        print(class_names)\n",
    "        files = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fileName = self.file_list[idx][2]\n",
    "        classCategory = self.file_list[idx][0]\n",
    "        im = Image.open(fileName)\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "# #         return im.view(3, 448, 448), classCategory\n",
    "        return im.view(3, 224, 224), classCategory\n",
    "# #         return im.view(3, 299, 299), classCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0jldgqi8aqT"
   },
   "outputs": [],
   "source": [
    "train_data = CassavaDataset(data_path, transform=train_transforms)\n",
    "\n",
    "test_data = CassavaDataset(test_path, transform=test_transforms)\n",
    "\n",
    "extraimage_data = CassavaDataset(extraimage_path, transform=train_transforms) #maybe need an other trasforms, I had to change the dataset structure :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHxCbEnY_NH7"
   },
   "source": [
    "# Over sample minority class and under sample majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLKDt9r38dGZ"
   },
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# valid_sampler = SubsetRandomSampler(val_indices)\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "#-----------------------------------------------------------------------------z\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "ipdb.set_trace()\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "# get target\n",
    "\n",
    "targets=[]\n",
    "#data=[]\n",
    "\n",
    "for i in train_data.file_list:\n",
    "    targets.append(i[0])\n",
    "   # data.append(i[2])\n",
    "    \n",
    "target_train=targets[split:]\n",
    "train_set=data[split:]\n",
    "print(len(train_set))\n",
    "\n",
    "target_test=targets[:split]\n",
    "#test_set=data[:split]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# for i in test_data.file_list:\n",
    "#     test_targets.append(i[0])\n",
    "    \n",
    "# target = torch.cat((torch.zeros(int(len(train_data) * 0.99), dtype=torch.long),\n",
    "#                     torch.ones(int(len(train_data) * 0.01), dtype=torch.long)))\n",
    "\n",
    "\n",
    "#count classes\n",
    "class_count = np.unique(target_train ,return_counts=True)[1]\n",
    "#print(class_count)\n",
    "class_count_test= np.unique(target_test, return_counts=True)[1]\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# get weights train\n",
    "\n",
    "weight_train = 1. / class_count\n",
    "#print(targets)\n",
    "samples_weight_train = weight_train[target_train]\n",
    "#print(samples_weight_train )\n",
    "\n",
    "samples_weight_train = torch.from_numpy(samples_weight_train)\n",
    "#print(samples_weight_train)\n",
    "sampler_train = WeightedRandomSampler(samples_weight_train, len(samples_weight_train))\n",
    "\n",
    "#### valid\n",
    "\n",
    "\n",
    "\n",
    "weight_test = 1. / class_count_test\n",
    "#print(targets)\n",
    "samples_weight_test = weight_test[target_test]\n",
    "#print(samples_weight_train )\n",
    "\n",
    "samples_weight_test = torch.from_numpy(samples_weight_test)\n",
    "#print(samples_weight_train)\n",
    "sampler_valid = WeightedRandomSampler(samples_weight_test, len(samples_weight_test))\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "                                             sampler=sampler_train)\n",
    "\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "                                             sampler=sampler_valid)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data, batch_size=32,\n",
    "#                                              sampler=valid_sampler)\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkQefu0a8sz2"
   },
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3qezIkE85cj"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,input,n_class):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.conv=nn.Sequential(nn.Conv2d(input,32,5),\n",
    "                                nn.ReLU(),\n",
    "#                                 nn.BatchNorm2d(32),\n",
    "                                nn.MaxPool2d(3),\n",
    "                                nn.Conv2d(32,64,3),\n",
    "                                nn.ReLU(),\n",
    "#                                 nn.BatchNorm2d(64),\n",
    "                                nn.MaxPool2d(3),\n",
    "                                nn.Conv2d(64,128,3),\n",
    "                                nn.ReLU(),\n",
    "#                                 nn.BatchNorm2d(128),\n",
    "                                nn.MaxPool2d(3)\n",
    "                               )\n",
    "        \n",
    "        self.fc1=nn.Linear(7*7*128,512)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2=nn.Linear(512,1024)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3=nn.Linear(1024,256)\n",
    "        self.dropout3=nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4=nn.Linear(256,n_class)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        \n",
    "#         print('conv',x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "\n",
    "        x=self.fc1(x)\n",
    "        x=self.dropout1(F.relu(x))\n",
    "        x=self.fc2(x)\n",
    "        x=self.dropout2(F.relu(x))\n",
    "#         print('fc',x)\n",
    "        x=self.fc3(x)\n",
    "        x=self.dropout3(F.relu(x))\n",
    "        x=self.fc4(x)\n",
    "        \n",
    "#         print('out',x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZR2wwkh86yK"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jz-VT3wy9NT5"
   },
   "source": [
    " # Model whith cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOocz4mU9OCw"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model(3,5)\n",
    "model=model.to(device)\n",
    "lr = 1e-4  # 0.001\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "train_loader.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "x = torch.Tensor(class_weights_normalized)\n",
    "x = x.to(device)\n",
    "x = x\n",
    "x,class_distrbution\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs=30\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    n=0\n",
    "    loss_list = []\n",
    "    acc_list = []   \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "#         correct=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Run the forward pass\n",
    "        images, labels = data\n",
    "        #print(images.size())\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)   \n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        #print(total)\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        \n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/total)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "\n",
    "            \n",
    "#######################################################################################\n",
    "\n",
    "model.eval()\n",
    "val_loss = AverageMeter()\n",
    "val_acc = AverageMeter()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKBoSrKB9u6w"
   },
   "source": [
    "#  Model with focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr2q2fuQ9pS5"
   },
   "outputs": [],
   "source": [
    "def focalLoss( outputs,targets,w,alpha=1,gamma=4):\n",
    "    ce_loss = torch.nn.functional.cross_entropy(outputs, targets, reduction='none') # important to add reduction='none' to keep per-batch-item loss\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = (alpha * (1-pt)**gamma * ce_loss).mean() # mean over the batch\n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thAAvd6q937a"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#--------------------------------------------------------------------------------------#\n",
    "train_loader.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "x = torch.Tensor(class_weights_normalized)\n",
    "x = x.to(device)\n",
    "x = x\n",
    "x,class_distrbution\n",
    "\n",
    "#--------------------------------------------------------------------------------------#\n",
    "\n",
    "model = Model(3,5)\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    "lr = 1e-4   # 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs=30\n",
    "total_loss_train, total_acc_train = [],[]\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "\n",
    "    model.train()\n",
    "    n=0\n",
    "    loss_list = []\n",
    "    acc_list = []  \n",
    "     \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "#         correct=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Run the forward pass\n",
    "        images, labels = data\n",
    "        #print(images.size())\n",
    "\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)   \n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.size())\n",
    "        loss =  focalLoss(outputs, labels,x)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        #print(total)\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        \n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/total)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "\n",
    " model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(focalLoss(outputs, labels,x).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WeightedRandomSampler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
