{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "import pretrainedmodels\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../input/ammi-2020-convnets/train/train\"\n",
    "test_path = \"../input/ammi-2020-convnets/test/test\"\n",
    "extraimage_path = \"../input/ammi-2020-convnets/extraimages/extraimages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set:')\n",
    "class_distrbution = {}\n",
    "for cls in os.listdir(data_path):\n",
    "    print('{}:{}'.format(cls, len(os.listdir(os.path.join(data_path, cls)))))\n",
    "    class_distrbution[cls] =  len(os.listdir(os.path.join(data_path, cls)))\n",
    "im = Image.open(data_path+'/cgm/train-cgm-738.jpg')\n",
    "print(im.size)\n",
    "class_distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for both the training and testing data\n",
    "mean=[0.4543, 0.5137, 0.3240]\n",
    "std=[0.1949, 0.1977, 0.1661]\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(331), #448, 299, 224, 331\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean,std=std)])\n",
    "\n",
    "test_transforms = transforms.Compose([ transforms.Resize(331),\n",
    "                                       transforms.CenterCrop(331),\n",
    "                                       transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean,std=std)])\n",
    "\n",
    "# normalize = transforms.Normalize(mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.classes = os.listdir(path)\n",
    "        self.path = [f\"{path}/{className}\" for className in self.classes]\n",
    "        self.file_list = [glob.glob(f\"{x}/*\") for x in self.path]\n",
    "        self.transform = transform\n",
    "\n",
    "        files = []\n",
    "        class_names = {}\n",
    "        for i, className in enumerate(self.classes):\n",
    "            for fileName in self.file_list[i]:\n",
    "                files.append([i, className, fileName])\n",
    "\n",
    "                name = str(i)+'-'+className\n",
    "                if name not in class_names:\n",
    "                    class_names[name] = 1\n",
    "                else:\n",
    "                    class_names[name] += 1\n",
    "        self.file_list = files\n",
    "        print(class_names)\n",
    "        files = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fileName = self.file_list[idx][2]\n",
    "        classCategory = self.file_list[idx][0]\n",
    "        im = Image.open(fileName)\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "# #         return im.view(3, 448, 448), classCategory\n",
    "#         return im.view(3, 224, 224), classCategory\n",
    "# #         return im.view(3, 299, 299), classCategory\n",
    "        return im.view(3, 331, 331), classCategory   # NASNetLarge 331x331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CassavaDataset(data_path, transform=train_transforms)\n",
    "\n",
    "test_data = CassavaDataset(test_path, transform=test_transforms)\n",
    "\n",
    "extraimage_data = CassavaDataset(extraimage_path, transform=train_transforms) #maybe need an other trasforms, I had to change the dataset structure :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128,\n",
    "                                             sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=128,\n",
    "                                             sampler=valid_sampler)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data,label in valid_loader:\n",
    "#     print(label)\n",
    "#     break\n",
    "# #     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(labels)\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "matplotlib_imshow(img_grid, one_channel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Additional(nn.Module):\n",
    "    def __init__(self, modelA,in_features,nb_classes=5):\n",
    "        super(Additional, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        # Remove last linear layer\n",
    "        self.modelA.last_linear = nn.Identity()\n",
    "        \n",
    "        for p in self.modelA.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # Create new classifier\n",
    "        self.fc_1 = nn.Linear(51200,256)\n",
    "        self.fc_2 = nn.Linear(256,  512)\n",
    "        self.fc_out = nn.Linear( 512, nb_classes)\n",
    "        \n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #model\n",
    "        x = self.modelA(x.clone())  \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #FC\n",
    "        x  = self.dropout(F.relu(self.fc_1(x)))\n",
    "        x = self.dropout(F.relu(self.fc_2(x)))\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used during training process, to calculation the loss and accuracy\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train, total_acc_train = [],[]\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        N = images.size(0)\n",
    "        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "    return train_loss.avg, train_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, optimizer, epoch):\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,train_loader, model):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    _class_labels = np.array(train_loader.dataset.classes)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, _ = data\n",
    "            images = Variable(images).to(device)\n",
    "    \n",
    "            outputs = model(images)\n",
    "    \n",
    "            prediction = outputs.data.cpu().numpy().argmax()\n",
    "            \n",
    "            _predicted_class_labels = _class_labels[prediction]\n",
    "            \n",
    "            pred.append(_predicted_class_labels)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = pretrainedmodels.se_resnet101(num_classes=1000, pretrained='imagenet')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fits = model.last_linear.in_features\n",
    "num_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Additional(model,num_fits)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.classes,\n",
    "class_weights = [class_distrbution[i] for i in train_loader.dataset.classes]\n",
    "class_weights_normalized = [max(class_weights)/i for i in class_weights]\n",
    "\n",
    "class_weights_normalized,torch.Tensor(class_weights_normalized)\n",
    "\n",
    "x = torch.Tensor(class_weights_normalized)\n",
    "x = x.to(device)\n",
    "x = x\n",
    "x,class_distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 2e-4 # 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight = x)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "epoch_num = 15\n",
    "best_val_acc = 0.88\n",
    "total_loss_val, total_acc_val = [],[]\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    loss_val, acc_val = validate(valid_loader, model, criterion, optimizer, epoch)\n",
    "    total_loss_val.append(loss_val)\n",
    "    total_acc_val.append(acc_val)\n",
    "    if acc_val > best_val_acc:\n",
    "        best_val_acc = acc_val\n",
    "        torch.save(model.state_dict(), model_name+'freeze_'+str(best_val_acc)[:4]+'.ckpt')\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load better model\n",
    "# model.load_state_dict(torch.load('se_resnext101_32x4dfreeze_0.86.ckpt'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Time Augmentation TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/qubvel/ttach\n",
    "\n",
    "import ttach as tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.Rotate90(angles=[0, 180]),\n",
    "#         tta.Scale(scales=[1, 2, 4]),\n",
    "#         tta.Multiply(factors=[0.9, 1, 1.1]),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "tta_model = tta.ClassificationTTAWrapper(model, transforms)\n",
    "\n",
    "loss_val, acc_val = validate(valid_loader, tta_model, criterion, optimizer, 5)\n",
    "\n",
    "# # #---------------------------------------------\n",
    "\n",
    "# lr = 2e-4 # 0.001\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# epoch_num = 5\n",
    "# best_val_acc = 0.88\n",
    "# total_loss_val, total_acc_val = [],[]\n",
    "\n",
    "# for epoch in range(1, epoch_num+1):\n",
    "#     loss_train, acc_train = train(train_loader, tta_model, criterion, optimizer, epoch)\n",
    "#     loss_val, acc_val = validate(valid_loader, tta_model, criterion, optimizer, epoch)\n",
    "#     total_loss_val.append(loss_val)\n",
    "#     total_acc_val.append(acc_val)\n",
    "#     if acc_val > best_val_acc:\n",
    "#         best_val_acc = acc_val\n",
    "#         torch.save(model.state_dict(), model_name+'freeze_'+str(best_val_acc)[:4]+'.ckpt')\n",
    "#         print('*****************************************************')\n",
    "#         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "#         print('*****************************************************')\n",
    "        \n",
    "# # tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val, acc_val = validate(train_loader, tta_model, criterion, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 100\n",
    "T2 = 700\n",
    "af = 3\n",
    "\n",
    "def alpha_weight(epoch):\n",
    "    if epoch < T1:\n",
    "        return 0.0\n",
    "    elif epoch > T2:\n",
    "        return af\n",
    "    else:\n",
    "         return ((epoch-T1) / (T2-T1))*af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train, total_acc_train = [],[]\n",
    "def semi_superv_train(train_loader, model, criterion, optimizer, unlabeled_loader, valid_loader, epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(unlabeled_loader):\n",
    "            images, _ = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg\n",
    "\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for i, data in enumerate(unlabeled_loader):\n",
    "        images, labels = data\n",
    "        N = images.size(0)\n",
    "        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
    "        images = Variable(images).to(device)\n",
    "#         labels = Variable(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "    return train_loss.avg, train_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept from : https://github.com/peimengsui/semi_supervised_mnist\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "acc_scores = []\n",
    "unlabel = []\n",
    "pseudo_label = []\n",
    "\n",
    "alpha_log = []\n",
    "test_acc_log = []\n",
    "test_loss_log = []\n",
    "\n",
    "best_val_acc = 0.87\n",
    "\n",
    "def semisup_train(train_loader, model, criterion, optimizer, unlabeled_loader, valid_loader, epoch):\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "#     EPOCHS = 5\n",
    "    \n",
    "    # Instead of using current epoch we use a \"step\" variable to calculate alpha_weight\n",
    "    # This helps the model converge faster\n",
    "    step = 100 \n",
    "    \n",
    "    model.train()\n",
    "    # for epoch in tqdm(range(EPOCHS)):\n",
    "    for epoch in range(epoch):\n",
    "\n",
    "#         for batch_idx, x_unlabeled in enumerate(unlabeled_loader):\n",
    "        for i, x_unlabeled in enumerate(unlabeled_loader):\n",
    "            \n",
    "            \n",
    "            # Forward Pass to get the pseudo labels\n",
    "            x_unlabeled = x_unlabeled[0].to(device)\n",
    "            \n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output_unlabeled = model(x_unlabeled)\n",
    "                pseudo_labeled = output_unlabeled.max(1, keepdim=True)[1]\n",
    "                \n",
    "                \n",
    "            model.train()\n",
    "            \n",
    "            # Now calculate the unlabeled loss using the pseudo label\n",
    "            output = model(x_unlabeled)\n",
    "            \n",
    "            pseudo_labeled = Variable(pseudo_labeled).to(device)\n",
    "            output = Variable(output).to(device)\n",
    "            \n",
    "            unlabeled_loss = alpha_weight(step) * F.nll_loss(output, pseudo_labeled).tem()   \n",
    "            \n",
    "            # Backpropogate\n",
    "            optimizer.zero_grad()\n",
    "            unlabeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # For every 50 batches train one epoch on labeled data \n",
    "            if i % 50 == 0:\n",
    "                \n",
    "                # Normal training procedure\n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                    \n",
    "                    X_batch = Variable(X_batch).to(device)\n",
    "                    y_batch = Variable(y_batch).to(device)\n",
    "                    \n",
    "                    output = model(X_batch)\n",
    "                    \n",
    "                    labeled_loss = F.nll_loss(output, y_batch)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    labeled_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Now we increment step by 1\n",
    "                step += 1\n",
    "                \n",
    "\n",
    "        loss_val, acc_va = validate(val_loader, model, criterion, optimizer, epoch) # evaluate(model, test_loader)\n",
    "        \n",
    "        print('Epoch: {} : Alpha Weight : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, alpha_weight(step), test_acc, test_loss))\n",
    "        \n",
    "        if acc_va > best_val_acc:\n",
    "            best_val_acc = acc_val\n",
    "            torch.save(model.state_dict(), model_name+'freeze_'+str(best_val_acc)[:4]+'.ckpt')\n",
    "            print('*****************************************************')\n",
    "            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "            print('*****************************************************')\n",
    "        \n",
    "#         \"\"\" LOGGING VALUES \"\"\"\n",
    "#         alpha_log.append(alpha_weight(step))\n",
    "#         test_acc_log.append(test_acc/100)\n",
    "#         test_loss_log.append(test_loss)\n",
    "#         \"\"\" ************** \"\"\"\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semisup_train(train_loader, model, criterion, optimizer, unlabeled_loader, valid_loader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept from : https://github.com/peimengsui/semi_supervised_mnist\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "acc_scores = []\n",
    "unlabel = []\n",
    "pseudo_label = []\n",
    "\n",
    "alpha_log = []\n",
    "test_acc_log = []\n",
    "test_loss_log = []\n",
    "\n",
    "best_val_acc = 0.87\n",
    "\n",
    "def semisup_train(model, train_loader, unlabeled_loader, val_loader):\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "    EPOCHS = 5\n",
    "    \n",
    "    # Instead of using current epoch we use a \"step\" variable to calculate alpha_weight\n",
    "    # This helps the model converge faster\n",
    "    step = 100 \n",
    "    \n",
    "    model.train()\n",
    "    # for epoch in tqdm(range(EPOCHS)):\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        for batch_idx, x_unlabeled in enumerate(unlabeled_loader):\n",
    "            \n",
    "            \n",
    "            # Forward Pass to get the pseudo labels\n",
    "            x_unlabeled = x_unlabeled[0].to(device)\n",
    "            model.eval()\n",
    "            output_unlabeled = model(x_unlabeled)\n",
    "            _, pseudo_labeled = torch.max(output_unlabeled, 1)\n",
    "            model.train()\n",
    "            \n",
    "            \n",
    "            \"\"\" ONLY FOR VISUALIZATION\"\"\"\n",
    "            if (batch_idx < 3) and (epoch % 10 == 0):\n",
    "                unlabel.append(x_unlabeled.cpu())\n",
    "                pseudo_label.append(pseudo_labeled.cpu())\n",
    "            \"\"\" ********************** \"\"\"\n",
    "            \n",
    "            # Now calculate the unlabeled loss using the pseudo label\n",
    "            output = model(x_unlabeled)\n",
    "            unlabeled_loss = alpha_weight(step) * criterion(output, pseudo_labeled)   \n",
    "            \n",
    "            # Backpropogate\n",
    "            optimizer.zero_grad()\n",
    "            unlabeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # For every 2 batches train one epoch on labeled data \n",
    "            if batch_idx % 2 == 0:\n",
    "                \n",
    "                # Normal training procedure\n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    output = model(X_batch)\n",
    "                    labeled_loss = criterion(output, y_batch)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    labeled_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Now we increment step by 1\n",
    "                step += 1\n",
    "                \n",
    "\n",
    "        loss_val, acc_va = validate(val_loader, model, criterion, optimizer, epoch) # evaluate(model, test_loader)\n",
    "        \n",
    "        print('Epoch: {} : Alpha Weight : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, alpha_weight(step), test_acc, test_loss))\n",
    "        \n",
    "        if acc_va > best_val_acc:\n",
    "            best_val_acc = acc_val\n",
    "            torch.save(model.state_dict(), model_name+'freeze_'+str(best_val_acc)[:4]+'.ckpt')\n",
    "            print('*****************************************************')\n",
    "            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "            print('*****************************************************')\n",
    "        \n",
    "        \"\"\" LOGGING VALUES \"\"\"\n",
    "        alpha_log.append(alpha_weight(step))\n",
    "        test_acc_log.append(test_acc/100)\n",
    "        test_loss_log.append(test_loss)\n",
    "        \"\"\" ************** \"\"\"\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semisup_train(model, train_loader, unlabeled_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train, total_acc_train = [],[]\n",
    "def train(train_loader, model, criterion, optimizer, unlabeled_loader, valid_loader, epoch):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        N = images.size(0)\n",
    "        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "    return train_loss.avg, train_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,label in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0:'cbsd', 1: 'cgm', 2: 'cbb', 3: 'healthy', 4: 'cmd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_dir):\n",
    "    # Process a PIL image for use in a PyTorch model\n",
    "    # tensor.numpy().transpose(1, 2, 0)\n",
    "    image = Image.open(image_dir)\n",
    "    preprocess = transforms.Compose([ transforms.Resize(224),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean,std=std)])\n",
    "    image = preprocess(image)\n",
    "    # Convert 2D image to 1D vector\n",
    "    image = np.expand_dims(image, 0)\n",
    "    image = torch.from_numpy(image)\n",
    "    inputs = image.to(device)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our model to predict the label\n",
    "def predict(image, model):\n",
    "    # Pass the image through our model\n",
    "    output = model(image)\n",
    "    # Reverse the log function in our output\n",
    "    output = torch.exp(output)\n",
    "    # Get the top predicted class, and the output percentage for\n",
    "    # that class\n",
    "    probs, classes = output.topk(1, dim=1)\n",
    "    return probs.item(), classes.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directory = \"./data/test/test/0\"\n",
    "predictions, test_image_fileName = [], []\n",
    "try:\n",
    "    test_images = listdir(test_directory)\n",
    "    for images in test_images:\n",
    "        test_image_fileName.append(images)\n",
    "        image = process_image(f'{test_directory}/{images}')\n",
    "        top_prob, top_class = predict(image, model)\n",
    "        predictions.append(class_names[top_class])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Creating pandas dataframe\")\n",
    "submission_data = {\"Category\":predictions,\"Id\":test_image_fileName,}\n",
    "submission_data_frame = pd.DataFrame(submission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data_frame.to_csv('submission'+model_name+'_freeze_86_flip.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
